{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurav714/Simple-python-practice/blob/main/Copy_of_Building_a_Simple_AI_Agent_with_Open_AI%E2%80%99s_gpt_oss_20b_powered_by_NVIDIA_NIM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/CW48eG5.png)\n",
        "\n",
        "# Building an AI Agent with OpenAI's gpt-oss-20b Powered by NVIDIA NIM\n",
        "\n",
        "In the following notebook, we'll take a look at a few things:\n",
        "\n",
        "1. How to use the NVIDIA-hosted NIM API to run inference on the model through the [Responses API](https://platform.openai.com/docs/api-reference/responses)\n",
        "2. How to use the NVIDIA-hosted NIM API to run inference on the model through the [ChatCompletions API](https://platform.openai.com/docs/api-reference/chat)\n",
        "3. How to build a Simple Web Search Agent powered by the `gpt-oss-20b` NIM using the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python/tree/main).\n",
        "\n",
        "Let's get right into it!"
      ],
      "metadata": {
        "id": "0jFwCpFmwgjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting an API Key from build.nvidia.com\n",
        "\n",
        "In order to use the NVIDIA-hosted NIM API - we'll need to have an API key from build.nvidia.com, luckily this is very straightforward.\n",
        "\n",
        "First, let's navigate to the model on [build.nvidia.com](https://build.nvidia.com/openai/gpt-oss-20b)!\n",
        "\n",
        "> NOTE: You'll need to ensure you're logged in before moving to the next steps!\n",
        "\n",
        "Once there, you can click on the green button that says: `View Code`.\n",
        "\n",
        "![image](https://i.imgur.com/mSGQPfC.png)\n",
        "\n",
        "A new modal should appear on your screen, where you can click the `Generate API Key` text to obtain your API key!\n",
        "\n",
        "![image](https://i.imgur.com/9ipSdPw.png)\n",
        "\n",
        "Once you have that API key, you're good to move on to the next step which will capture it as an environment variable."
      ],
      "metadata": {
        "id": "LjSo-Kx3xzQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiwQ8aZUUJpv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the OpenAI Library with the NVIDIA-hosted NIM API API\n",
        "\n",
        "We will be using the Python [OpenAI SDK](https://github.com/openai/openai-python) to access the `gpt-oss-20b` model on build.nvidia.com.\n",
        "\n",
        "Let's start with a classic `pip install`."
      ],
      "metadata": {
        "id": "C7anNSsM0ZNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI2c_q34Uqke",
        "outputId": "e6576b1e-8e8f-4da7-a291-7f9d0d266f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/785.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m778.2/785.8 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.8/785.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've installed the `openai` library - we can use it to create an OpenAI client, which we'll point at the NVIDIA-hosted NIM API endpoint.\n",
        "\n",
        "We'll also be sure to provide the API key we entered above by referencing the environment variable."
      ],
      "metadata": {
        "id": "N8oxujI-07oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = os.environ[\"NVIDIA_API_KEY\"]\n",
        ")"
      ],
      "metadata": {
        "id": "lrzOTsg2UZPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with the Responses API which is powered by the new [Harmony](https://github.com/openai/harmony) response format.\n",
        "\n",
        "> NOTE: This is something that NVIDIA NIM handles for users, so you can continue to use the APIs you're used to using without interruption.\n",
        "\n",
        "Let's start with an easy question:\n",
        "\n",
        "\"How many 'r's in strawberry?\".\n",
        "\n",
        "Notice that through the Responses API we can control things like the model's reasoning effort just as you would expect!\n",
        "\n",
        "> NOTE: Reasoning efforts are available in `low`, `medium`, and `high` for this model.\n",
        "\n",
        "We're going to otherwise use the suggest defaults for our inference parameters, and set our `max_tokens` to a conservative `4096` - though the model itself supports a context length of up to `128K` tokens.\n",
        "\n",
        "We'll also enable streaming, so we can see our streamed response as it's generated!\n",
        "\n",
        "> NOTE: In the parsing of the response - we're able to specifically parse the reasoning vs. final output tokens!"
      ],
      "metadata": {
        "id": "8nV95Jaq1Qa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strawberry_prompt = \"\"\"\n",
        "How many 'r's in strawberry?\n",
        "\"\"\"\n",
        "\n",
        "response = client.responses.create(\n",
        "  model=\"openai/gpt-oss-20b\",\n",
        "  input=[strawberry_prompt],\n",
        "  reasoning={\"effort\" : \"low\"},\n",
        "  max_output_tokens=4096,\n",
        "  top_p=0.7,\n",
        "  temperature=0.6,\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "reasoning_done = False\n",
        "for chunk in response:\n",
        "  if chunk.type == \"response.reasoning_text.delta\":\n",
        "    print(chunk.delta, end=\"\")\n",
        "  elif chunk.type == \"response.output_text.delta\":\n",
        "    if not reasoning_done:\n",
        "      print(\"\\n\")\n",
        "      reasoning_done = True\n",
        "    print(chunk.delta, end=\"\")"
      ],
      "metadata": {
        "id": "UE83JYDyUxFq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "e228e8be-f24b-4e7d-80a6-b11714f5c96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'status': 401, 'title': 'Unauthorized', 'detail': 'Authentication failed'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1878946706.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m response = client.responses.create(\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openai/gpt-oss-20b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstrawberry_prompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/responses/responses.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, verbosity, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[0;32m--> 811\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    812\u001b[0m             \u001b[0;34m\"/responses\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'status': 401, 'title': 'Unauthorized', 'detail': 'Authentication failed'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the reasoning length may vary between runs - the model is able to accurately asses that there are 3 'r's in 'strawberry' in `low` reasoning mode.\n",
        "\n",
        "Let's try a more difficult example in `high` reasoning mode.\n",
        "\n",
        "We'll use an example from AIME25, a dataset designed to test reasoning model's Mathematics capabilities.\n",
        "\n",
        "The example is as follows (small formatting tweaks for readability not present in the prompt):\n",
        "\n",
        "```\n",
        "The 9 members of a baseball team went to an ice cream parlor after their game.\n",
        "Each player had a singlescoop cone of chocolate, vanilla, or strawberry ice cream.\n",
        "At least one player chose each flavor, and the number of players who chose chocolate was greater than the number of players\n",
        "who chose vanilla, which was greater than the number of players who chose strawberry.\n",
        "Let $N$ be the number of different assignments of flavors to players that meet these conditions.\n",
        "Find the remainder when $N$ is divided by 1000.\n",
        "```\n",
        "\n",
        "We expect that the model will arrive at the correct response of `16`.\n"
      ],
      "metadata": {
        "id": "MkWaEGlR3MHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "math_prompt = \"\"\"\n",
        "The 9 members of a baseball team went to an ice cream parlor after their game. Each player had a singlescoop cone of chocolate, vanilla, or strawberry ice cream. At least one player chose each flavor, and the number of players who chose chocolate was greater than the number of players who chose vanilla, which was greater than the number of players who chose strawberry. Let $N$ be the number of different assignments of flavors to players that meet these conditions. Find the remainder when $N$ is divided by 1000.\n",
        "\n",
        "Please provide your answer in boxed format.\n",
        "\"\"\"\n",
        "\n",
        "response = client.responses.create(\n",
        "  model=\"openai/gpt-oss-20b\",\n",
        "  input=[math_prompt],\n",
        "  reasoning={\"effort\" : \"high\"},\n",
        "  max_output_tokens=16384,\n",
        "  top_p=0.7,\n",
        "  temperature=0.6,\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "reasoning_done = False\n",
        "for chunk in response:\n",
        "  if chunk.type == \"response.reasoning_text.delta\":\n",
        "    print(chunk.delta, end=\"\")\n",
        "  elif chunk.type == \"response.output_text.delta\":\n",
        "    if not reasoning_done:\n",
        "      print(\"\\n\")\n",
        "      reasoning_done = True\n",
        "    print(chunk.delta, end=\"\")"
      ],
      "metadata": {
        "id": "w8IHXDc4p0jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not only can we see that the model got the question correct - but we can also see an increase in the number of reasoning tokens used to arrive at the correct response!"
      ],
      "metadata": {
        "id": "rT-a7SLS4sym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Simple Web Search Agent Powered by the `gpt-oss-20b` NIM.\n",
        "\n",
        "Next, we'll look at a very simple example of how we can build Agents leveraging the NVIDIA-hosted NIM API powered by NVIDIA NIM.\n",
        "\n",
        "In order to get started, we need to grab both the `openai-agents`, and `tavily-python` library.\n",
        "\n",
        "> NOTE: This example will require a Tavily API key, which you can obtain using the process outlined [here](https://docs.tavily.com/documentation/quickstart#get-your-free-tavily-api-key).\n",
        "\n",
        "We will be using the ChatCompletion endpoint for this example due to the fact that the Responses API does not support tool-calling on the build.nvidia.com NVIDIA-hosted NIM API at this time.\n",
        "\n",
        "> NOTE: The NIM container that you can download, deploy and run locally *does* support tool-calling in the Responses API. Instructions on how you can download and run the NIM are available [here](https://build.nvidia.com/openai/gpt-oss-20b/deploy)!"
      ],
      "metadata": {
        "id": "7kk-fkLQ40Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai-agents tavily-python"
      ],
      "metadata": {
        "id": "6dynEYLiU3gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your TAVILY API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wpJz3MHjoG5",
        "outputId": "6db081b8-a35b-48d4-d784-1eab0f9fb042"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your TAVILY API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can create our `AsyncOpenAI` client through the same process we used for our `OpenAI` client."
      ],
      "metadata": {
        "id": "m4MrPd4W5nGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AsyncOpenAI\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = os.environ[\"NVIDIA_API_KEY\"]\n",
        ")"
      ],
      "metadata": {
        "id": "1d24oWmGVQuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we need to search the web - we will create a `function_tool` that takes in a search query, and returns the results in a newline separated list."
      ],
      "metadata": {
        "id": "5DWGSWkN5u_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import function_tool\n",
        "from tavily import TavilyClient\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "@function_tool\n",
        "async def search_web(query: str) -> str:\n",
        "    \"\"\"Search the web for the given query.\n",
        "\n",
        "    Args:\n",
        "      query: The query to search for.\n",
        "    \"\"\"\n",
        "    print(f\"[INFO] Searching: {query}\")\n",
        "    response = tavily_client.search(query)\n",
        "    return \"\\n\".join([f\"{result.title}: {result.content}\" for result in response.results])"
      ],
      "metadata": {
        "id": "mPWHDzD4kQB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up, we'll create the actual Agent itself. You can read more about the OpenAI Agents SDK [here](https://github.com/openai/openai-agents-python/tree/main), we'll be focusing on the integration with the NVIDIA-hosted NIM API today.\n",
        "\n",
        "The following parameter is what allows the integration to work smoothly.\n",
        "\n",
        "```\n",
        "OpenAIChatCompletionsModel(model=\"openai/gpt-oss-20b\", ...)\n",
        "```\n",
        "\n",
        "Since NVIDIA NIM exposes the ChatCompletions API for the model - you can seamlessly integrate these models whether locally or through the NVIDIA-hosted NIM API."
      ],
      "metadata": {
        "id": "3frz3WRq8TcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, OpenAIChatCompletionsModel\n",
        "\n",
        "agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You're a helpful assistant. You respond in a format that is useful for Enterprise Executives.\",\n",
        "        model=OpenAIChatCompletionsModel(model=\"openai/gpt-oss-20b\", openai_client=client),\n",
        "        tools=[search_web],\n",
        ")"
      ],
      "metadata": {
        "id": "gAXUJiS3VZ_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our Agent, let's go ahead and *disable* automated tracing - since we're not providing our OpenAI key, and are only communicating with the NVIDIA-hosted NIM API powered by NIM on build.nvidia.com."
      ],
      "metadata": {
        "id": "LmTuo6PP-fJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import set_tracing_disabled\n",
        "set_tracing_disabled(disabled=True)"
      ],
      "metadata": {
        "id": "Q9vVAYcarp5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can run our agent and see how it responds to a question about the enterprise benefits of NVIDIA NIM!"
      ],
      "metadata": {
        "id": "Oi5gC5VD-rA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Runner\n",
        "\n",
        "result = await Runner.run(agent, \"Briefly describe the enterprise benefits of NVIDIA NIM. 200 words or less.\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB2I-OmqVfiv",
        "outputId": "78b01309-6e87-4b40-a1b7-a375cf244e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Searching: NVIDIA NIM enterprise benefits\n",
            "[INFO] Searching: NVIDIA NIM enterprise benefits\n",
            "**Enterprise benefits of NVIDIA NIM (NVIDIA Inference Model)** – <200 words  \n",
            "\n",
            "1. **Rapid deployment** – NIMs are pre‑built, GPU‑optimized inference pipelines that let teams spin up large language, vision, or multimodal models in minutes, cutting time‑to‑market.  \n",
            "\n",
            "2. **Scalable performance** – Built on NVIDIA’s Triton Inference Server and GPU‑cloud stack, NIMs auto‑scale with demand, delivering consistent low‑latency inference for millions of requests.  \n",
            "\n",
            "3. **Cost efficiency** – Optimized kernels and mixed‑precision execution reduce GPU utilization and power consumption, lowering CAPEX and OPEX for on‑prem or cloud deployments.  \n",
            "\n",
            "4. **Unified management** – A single API and dashboard control model lifecycle, monitoring, and logging, simplifying operations and reducing DevOps overhead.  \n",
            "\n",
            "5. **Security & compliance** – NIMs run inside isolated containers, support role‑based access, and integrate with enterprise IAM and audit tools, easing GDPR, HIPAA, or SOC‑2 compliance.  \n",
            "\n",
            "6. **Ecosystem integration** – Seamlessly plug into NVIDIA AI Enterprise, Kubernetes, and existing data pipelines, enabling hybrid‑cloud or edge‑to‑cloud strategies without rewriting code.  \n",
            "\n",
            "Overall, NVIDIA NIM turns complex AI workloads into turnkey, high‑performance services that save time, cut costs, and keep enterprises secure and compliant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've built a simple Agent using NVIDIA NIM - head over the [model page](https://build.nvidia.com/openai/gpt-oss-20b) and try it out, or follow the [deployment instructions](https://build.nvidia.com/openai/gpt-oss-20b/deploy) to start building locally!"
      ],
      "metadata": {
        "id": "AFlXkbqa-yuZ"
      }
    }
  ]
}